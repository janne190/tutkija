# SPDX-FileCopyrightText: 2024-present tutkija <tutkija@tutkija.fi>
#
# SPDX-License-Identifier: MIT

from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any

from pydantic import BaseModel, Field, HttpUrl


class Provider(str, Enum):
    """Supported LLM providers."""

    openai = "openai"
    gemini = "gemini"


class RunConfig(BaseModel):
    """Configuration for a single research run."""

    topic: str = Field(..., description="The research topic.")
    provider: Provider = Field(Provider.gemini, description="LLM provider to use.")
    model: str = Field("gemini-1.5-pro", description="LLM model to use.")
    budget_usd: float = Field(3.0, description="Maximum USD budget for LLM calls.")
    max_iterations: int = Field(2, description="Maximum iterations for critic feedback loop.")
    top_k: int = Field(6, description="Top K results for retrieval.")
    bm25_k: int = Field(20, description="BM25 K value for retrieval.")
    questions: Optional[List[str]] = Field(
        None, description="List of questions to answer, or path to a questions.yaml file."
    )
    require_sources: int = Field(2, description="Minimum unique sources required per answer.")
    enforce_pages: bool = Field(True, description="Enforce page number validation for citations.")
    output_dir: Path = Field(
        ..., description="Directory to store run artifacts. Will be created if it doesn't exist."
    )


class NodeMetrics(BaseModel):
    """Metrics for a single node execution."""

    node_name: str
    start_time: datetime
    end_time: datetime
    duration_s: float
    llm_cost_usd: float = 0.0
    token_count: int = 0
    status: str = "success"
    error_message: Optional[str] = None


class CriticReport(BaseModel):
    """Report generated by the critic agent."""

    timestamp: datetime = Field(default_factory=datetime.now)
    overall_status: str = Field(..., description="Overall status: 'pass', 'pass_with_warnings', 'fail'.")
    findings: List[str] = Field(default_factory=list, description="List of specific findings/issues.")
    correction_suggestions: List[str] = Field(
        default_factory=list, description="Suggestions for correcting identified issues."
    )
    patch_file: Optional[Path] = Field(None, description="Path to the generated diff file.")
    qa_audit_file: Optional[Path] = Field(None, description="Path to the QA audit CSV file.")


class Patch(BaseModel):
    """Represents a unified diff patch."""

    original_file: Path
    patch_content: str


class RunState(BaseModel):
    """Represents the state of a research run."""

    run_id: str
    config: RunConfig
    current_node: Optional[str] = None
    plan: Optional[Dict[str, Any]] = None  # e.g., list of questions, budget, paths
    metrics: List[NodeMetrics] = Field(default_factory=list)
    critic_report: Optional[CriticReport] = None
    # Paths to key artifacts
    search_results_path: Optional[Path] = None
    screened_results_path: Optional[Path] = None
    parsed_index_path: Optional[Path] = None
    index_meta_path: Optional[Path] = None
    qa_results_path: Optional[Path] = None
    report_draft_path: Optional[Path] = None
    prisma_path: Optional[Path] = None
    # Other state variables
    iteration: int = 0
    total_llm_cost_usd: float = 0.0
    total_duration_s: float = 0.0
    errors: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    # For LangGraph, this might be more complex, e.g., a dict of node outputs
    node_outputs: Dict[str, Any] = Field(default_factory=dict)
